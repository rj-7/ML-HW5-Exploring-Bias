{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6361ec3-2edd-4c41-8ed8-2a72d9df920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb69e9-0484-4573-9a52-244bacec9342",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf06caca-f53c-4b96-a9a8-bd9b9898dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training and test folder paths\n",
    "training_path = 'features_train/features_train'\n",
    "test_path = 'features_test/features_test'\n",
    "\n",
    "#Load labels file\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "#Load feature description files, take out column 0 to use as header for training/test sets\n",
    "features = pd.read_csv('feature_description.csv', encoding_errors='ignore', header=None, index_col=0)\n",
    "features = features.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb8a6c5-8959-4820-a408-437f9089aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    #Init empty dataframe\n",
    "    res = pd.DataFrame()\n",
    "    for file in os.listdir(folder_path):\n",
    "        #for each speaker file\n",
    "        if file.endswith('.csv'):\n",
    "            #get participant id from filename, eg filename: 'spk_305.csv'\n",
    "            participant = float(file.split('_')[1].split('.')[0])\n",
    "            #find labels for the participant\n",
    "            label = labels[labels['Participant_ID'] == participant]\n",
    "            #load participant feature file\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            data_df = pd.read_csv(file_path, header=None, names=features)\n",
    "            #Add labels and participant id columns\n",
    "            data_df['participant'] = participant\n",
    "            data_df['gender'] = label['Gender'].values[0]\n",
    "            data_df['depression'] = label['Depression'].values[0]\n",
    "            #combine everything to result\n",
    "            res = pd.concat([res, data_df])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9f8ea7-287e-44b8-8b78-b25daaeadff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13626"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load training data\n",
    "training_df = load_data(training_path)\n",
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6652d62d-a35b-43a5-84c5-7c39d7fa8562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load test data\n",
    "test_df = load_data(test_path)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ae20c-5d3d-48d7-a64c-545a50a9c753",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2c085a-988f-416a-9abd-e2037a69d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value percent % for each column, total samples 13626\n",
      "Number of samples with missing values: 1\n",
      "F0semitoneFrom27.5Hz_sma3nz_amean             0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_stddevNorm        0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile20.0    0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile50.0    0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile80.0    0.007339\n",
      "                                                ...   \n",
      "StddevUnvoicedSegmentLength                   0.007339\n",
      "equivalentSoundLevel_dBp                      0.007339\n",
      "participant                                   0.000000\n",
      "gender                                        0.000000\n",
      "depression                                    0.000000\n",
      "Length: 91, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check Missing values\n",
    "missing_values = (training_df.isnull().sum()/len(training_df)) *100\n",
    "total_missing_values = training_df.isnull().any(axis=1).sum()\n",
    "print(f'Missing value percent % for each column, total samples {len(training_df)}')\n",
    "print(f'Number of samples with missing values: {total_missing_values}')\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccaa72fa-f720-4c61-ad92-a771e07c9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    #drop missing va;ues since there is just 1 sample\n",
    "    data_nona = data.dropna()\n",
    "    #Normalization z-score\n",
    "    scaler = StandardScaler()    \n",
    "    # Store the columns to keep for later concatenation\n",
    "    columns_to_keep = ['participant', 'gender', 'depression']\n",
    "    # Extract the columns to be scaled and drop them from the original DataFrame\n",
    "    temp = data_nona[columns_to_keep].copy()\n",
    "    data_nona.drop(columns=columns_to_keep, axis=1, inplace=True)\n",
    "    # Scale the remaining columns using StandardScaler and convert back to DataFrame\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data_nona), columns=data_nona.columns)\n",
    "    scaled_data.reset_index(drop=True, inplace=True)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    # Concatenate the scaled data with the columns we kept earlier\n",
    "    processed_data = pd.concat([scaled_data, temp], axis=1)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afcae2-ac22-4117-a04e-9434806d7569",
   "metadata": {},
   "source": [
    "## Methods to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1453e6-d155-47fc-b17a-03d19987f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates accuracy\n",
    "# pass true and predicted labels\n",
    "# return accuracy score\n",
    "def calculate_total_accuracy(true_labels, predicted_labels):\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculates accuracy\n",
    "# pass true and predicted labels\n",
    "# return balanced accuracy score\n",
    "def calculate_balanced_accuracy(true_labels, predicted_labels):\n",
    "    #calculkate confusion matrix\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    TP = matrix[1, 1]\n",
    "    TN = matrix[0, 0]\n",
    "    FP = matrix[0, 1]\n",
    "    FN = matrix[1, 0]\n",
    "    #For positive class, how many correct predictions\n",
    "    accuracy_positive = TP/(TP+FN)\n",
    "    #For negative class how many \n",
    "    accuracy_negative = TN/(TN+FP)\n",
    "    return 0.5*(accuracy_positive + accuracy_negative)\n",
    "\n",
    "#Calculates Equality of Opportunity\n",
    "# pass true and predicted labels for male samples\n",
    "# pass true and predicted labels for female samples\n",
    "# return balanced accuracy score\n",
    "def calculate_EO(true_labels_male, \n",
    "                 true_labels_female,\n",
    "                 predicted_labels_male,\n",
    "                predicted_labels_female):\n",
    "    #Calculate True pistive rate for male gender with confusion matrix\n",
    "    matrix_male = confusion_matrix(true_labels_male, predicted_labels_male)\n",
    "    TP = matrix_male[1, 1]\n",
    "    TN = matrix_male[0, 0]\n",
    "    FN = matrix_male[1, 0]\n",
    "    TPR_male = TP/(TP+FN)\n",
    "\n",
    "    #Calculate True pistive rate for female gender with confusion matrix\n",
    "    matrix_female = confusion_matrix(true_labels_female, predicted_labels_female)\n",
    "    TP = matrix_female[1, 1]\n",
    "    TN = matrix_female[0, 0]\n",
    "    FN = matrix_female[1, 0]\n",
    "    TPR_female = TP/(TP+FN)\n",
    "    \n",
    "    # Calculate EO\n",
    "    return 1-abs(TPR_male-TPR_female) \n",
    "\n",
    "#Function to calculate majority votings\n",
    "#Pass labels\n",
    "#Returns mode or which label was predicted most\n",
    "def majority_voting(df):\n",
    "    counts = df.value_counts()\n",
    "    return counts.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63781b8a-f603-4075-b8ba-a252f66bac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate all metrics\n",
    "#Pass true labels, predicted labels and a reference(test/val) dataframe\n",
    "#referece dataframe should have all labels and features\n",
    "#Returns a dictionary with all the metric calculated\n",
    "def calculate_metrics(y_true, y_pred, test_data):\n",
    "    # Initialize metrics\n",
    "    metrics = {}\n",
    "    #---------------------------------------------------CALCULATING TOTAL METRICS\n",
    "    #calculate total accuracy\n",
    "    metrics[\"Total accuracy\"] = calculate_total_accuracy(y_true, y_pred)\n",
    "    #calculate total balanced accuracy\n",
    "    metrics[\"Total Balanced accuracy\"] = calculate_balanced_accuracy(y_true, y_pred)\n",
    "    #calculate total EO\n",
    "    #find gender based indices for true labels from data\n",
    "    male_indices = test_data[test_data['gender']==1].index\n",
    "    female_indices = test_data[test_data['gender']==0].index\n",
    "    #separate true labels based on indices\n",
    "    male_true = y_true.loc[male_indices]\n",
    "    female_true = y_true.loc[female_indices]\n",
    "    #Find the corresponding indices for predicted labels from true_labels\n",
    "    male_true_index_list = male_true.index.tolist()\n",
    "    female_true_index_list = female_true.index.tolist()\n",
    "    #Get separated predicted labels based on gender\n",
    "    male_predicted = y_pred[[male_true_index_list.index(index) for index in male_true_index_list]]\n",
    "    female_predicted = y_pred[[female_true_index_list.index(index) for index in female_true_index_list]]\n",
    "    metrics[\"Total EO\"] = calculate_EO(male_true, female_true, male_predicted, female_predicted)\n",
    "    #-------------------------------------------------CALCULATING AGGREGATED METRICS FOR EACH PARTICIPANT\n",
    "    predictions_df = pd.DataFrame({'participant': test_data['participant'], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "    aggregated_y_true = predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "    aggregated_y_pred = predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "    #Calculate aggregated accuracy score\n",
    "    metrics[\"Aggregated accuracy score\"] = calculate_total_accuracy(aggregated_y_true, aggregated_y_pred)\n",
    "    #Calculate balanced aggregated accuracy\n",
    "    metrics[\"Aggregated balanced accuracy score\"] = calculate_balanced_accuracy(aggregated_y_true, aggregated_y_pred)\n",
    "    #Calculate aggregated EOs\n",
    "    male_predictions_df = pd.DataFrame({'participant': test_data['participant'].loc[male_indices], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "    male_aggregated_y_true = male_predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "    male_aggregated_y_pred = male_predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "    female_predictions_df = pd.DataFrame({'participant': test_data['participant'].loc[female_indices], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "    female_aggregated_y_true = female_predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "    female_aggregated_y_pred = female_predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "    metrics[\"Aggregated EO score\"] = calculate_EO(male_aggregated_y_true, female_aggregated_y_true, male_aggregated_y_pred, female_aggregated_y_pred)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2503eab-929d-4284-ac8b-b6e01732ebed",
   "metadata": {},
   "source": [
    "## Data Modeling - Depression Classification\n",
    "### What models to try?\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- TBD..........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f0769-2394-44e0-a91e-535db7b7e2e8",
   "metadata": {},
   "source": [
    "### Model attempt: Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2c4d21-e943-4705-a523-ff1c9d7796cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for depth 3\n",
      "{'Total accuracy': 0.739302752293578, 'Total Balanced accuracy': 0.5909240479132576, 'Total EO': 0.9504593123805108, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5857142857142856, 'Aggregated EO score': 0.8333333333333334}\n",
      "\n",
      "for depth 5\n",
      "{'Total accuracy': 0.7470825688073395, 'Total Balanced accuracy': 0.613583936397171, 'Total EO': 0.9324445056229891, 'Aggregated accuracy score': 0.7839080459770115, 'Aggregated balanced accuracy score': 0.6109126984126985, 'Aggregated EO score': 0.8833333333333332}\n",
      "\n",
      "for depth 7\n",
      "{'Total accuracy': 0.7635229357798166, 'Total Balanced accuracy': 0.665776640046492, 'Total EO': 0.9642291174994325, 'Aggregated accuracy score': 0.8160919540229885, 'Aggregated balanced accuracy score': 0.674404761904762, 'Aggregated EO score': 0.8166666666666667}\n",
      "\n",
      "for depth 9\n",
      "{'Total accuracy': 0.7695412844036696, 'Total Balanced accuracy': 0.6872769055678483, 'Total EO': 0.9774666212955376, 'Aggregated accuracy score': 0.8413793103448276, 'Aggregated balanced accuracy score': 0.7253968253968253, 'Aggregated EO score': 0.8333333333333333}\n",
      "\n",
      "for depth 15\n",
      "{'Total accuracy': 0.7610275229357798, 'Total Balanced accuracy': 0.7051558553708489, 'Total EO': 0.9909122978196259, 'Aggregated accuracy score': 0.9149425287356323, 'Aggregated balanced accuracy score': 0.8509920634920635, 'Aggregated EO score': 0.8166666666666667}\n",
      "\n",
      "for depth 30\n",
      "{'Total accuracy': 0.7548623853211008, 'Total Balanced accuracy': 0.7053823197066239, 'Total EO': 0.9793427856398935, 'Aggregated accuracy score': 0.9241379310344827, 'Aggregated balanced accuracy score': 0.8676587301587302, 'Aggregated EO score': 0.8833333333333334}\n",
      "\n",
      "for depth 50\n",
      "{'Total accuracy': 0.7535412844036697, 'Total Balanced accuracy': 0.7037251836232843, 'Total EO': 0.9762026020479124, 'Aggregated accuracy score': 0.9126436781609195, 'Aggregated balanced accuracy score': 0.8519841269841268, 'Aggregated EO score': 0.9}\n",
      "\n",
      "for depth 70\n",
      "{'Total accuracy': 0.756697247706422, 'Total Balanced accuracy': 0.7069980737965663, 'Total EO': 0.9793028545904108, 'Aggregated accuracy score': 0.9149425287356323, 'Aggregated balanced accuracy score': 0.8561507936507937, 'Aggregated EO score': 0.9166666666666666}\n",
      "\n",
      "for depth 90\n",
      "{'Total accuracy': 0.7550825688073394, 'Total Balanced accuracy': 0.7054974609744304, 'Total EO': 0.9819326606529352, 'Aggregated accuracy score': 0.92183908045977, 'Aggregated balanced accuracy score': 0.8686507936507937, 'Aggregated EO score': 0.9333333333333333}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------Tuning for different depths-------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "training_data = preprocess_data(training_df)\n",
    "X = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y = training_data['depression']\n",
    "\n",
    "# Define the depths to experiment with\n",
    "depths = [3, 5, 7, 9, 15, 30, 50, 70, 90]\n",
    "# Initialize metrics\n",
    "metrics = {}\n",
    "\n",
    "# Perform cross-validation for each tree depth\n",
    "for depth in depths:\n",
    "    # Initialize decision tree model\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "    # Cross validation k fold, 4:1::training:validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #list to store metrics for each cross validation split\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Perform cross-validation and collect metrics\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        #Fit the training set\n",
    "        tree.fit(X_train, y_train)\n",
    "        #Predict validation set\n",
    "        y_pred = tree.predict(X_val)\n",
    "        #calculate metrics\n",
    "        fold_metrics.append(calculate_metrics(y_val, y_pred, training_data.iloc[val_index]))\n",
    "\n",
    "    #find avg metrics for each depth\n",
    "    metrics[depth] = {}\n",
    "    sums = {}\n",
    "    for metric in fold_metrics:\n",
    "        for key, value in metric.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    for key in sums:\n",
    "        sums[key]/=len(fold_metrics)\n",
    "    metrics[depth] = sums\n",
    "\n",
    "#Print metrics for all the hyperparameters (Depth)\n",
    "for depth in metrics:\n",
    "    print(f\"for depth {depth}\")\n",
    "    print(metrics[depth])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f350c55a-6fa3-45e4-8c4e-8023129d953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total accuracy': 0.6871951219512196, 'Total Balanced accuracy': 0.5795540476430081, 'Total EO': 0.6996699669966997, 'Aggregated accuracy score': 0.7, 'Aggregated balanced accuracy score': 0.5476190476190477, 'Aggregated EO score': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------Testing for best depth----------------------------------------------------------------\n",
    "test_data = preprocess_data(test_df)\n",
    "training_data = preprocess_data(training_df)\n",
    "X_train = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_train = training_data['depression']\n",
    "X_test = test_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_test = test_data['depression']\n",
    "\n",
    "best_depth = 70\n",
    "# Initialize decision tree model\n",
    "tree = DecisionTreeClassifier(max_depth=best_depth, criterion='entropy')\n",
    "#Fit the training set\n",
    "tree.fit(X_train, y_train)\n",
    "#Predict for test set\n",
    "y_pred = tree.predict(X_test)\n",
    "#get metrics\n",
    "metrics = calculate_metrics(y_test, y_pred, test_data)\n",
    "\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
