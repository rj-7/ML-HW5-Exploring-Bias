{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6361ec3-2edd-4c41-8ed8-2a72d9df920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb69e9-0484-4573-9a52-244bacec9342",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf06caca-f53c-4b96-a9a8-bd9b9898dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training and test folder paths\n",
    "training_path = 'features_train/features_train'\n",
    "test_path = 'features_test/features_test'\n",
    "\n",
    "#Load labels file\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "#Load feature description files, take out column 0 to use as header for training/test sets\n",
    "features = pd.read_csv('feature_description.csv', encoding_errors='ignore', header=None, index_col=0)\n",
    "features = features.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb8a6c5-8959-4820-a408-437f9089aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    #Init empty dataframe\n",
    "    res = pd.DataFrame()\n",
    "    for file in os.listdir(folder_path):\n",
    "        #for each speaker file\n",
    "        if file.endswith('.csv'):\n",
    "            #get participant id from filename, eg filename: 'spk_305.csv'\n",
    "            participant = float(file.split('_')[1].split('.')[0])\n",
    "            #find labels for the participant\n",
    "            label = labels[labels['Participant_ID'] == participant]\n",
    "            #load participant feature file\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            data_df = pd.read_csv(file_path, header=None, names=features)\n",
    "            #Add labels and participant id columns\n",
    "            data_df['participant'] = participant\n",
    "            data_df['gender'] = label['Gender'].values[0]\n",
    "            data_df['depression'] = label['Depression'].values[0]\n",
    "            #combine everything to result\n",
    "            res = pd.concat([res, data_df])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9f8ea7-287e-44b8-8b78-b25daaeadff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13626"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load training data\n",
    "training_df = load_data(training_path)\n",
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6652d62d-a35b-43a5-84c5-7c39d7fa8562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load test data\n",
    "test_df = load_data(test_path)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ae20c-5d3d-48d7-a64c-545a50a9c753",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2c085a-988f-416a-9abd-e2037a69d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value percent % for each column, total samples 13626\n",
      "Number of samples with missing values: 1\n",
      "F0semitoneFrom27.5Hz_sma3nz_amean             0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_stddevNorm        0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile20.0    0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile50.0    0.007339\n",
      "F0semitoneFrom27.5Hz_sma3nz_percentile80.0    0.007339\n",
      "                                                ...   \n",
      "StddevUnvoicedSegmentLength                   0.007339\n",
      "equivalentSoundLevel_dBp                      0.007339\n",
      "participant                                   0.000000\n",
      "gender                                        0.000000\n",
      "depression                                    0.000000\n",
      "Length: 91, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check Missing values\n",
    "missing_values = (training_df.isnull().sum()/len(training_df)) *100\n",
    "total_missing_values = training_df.isnull().any(axis=1).sum()\n",
    "print(f'Missing value percent % for each column, total samples {len(training_df)}')\n",
    "print(f'Number of samples with missing values: {total_missing_values}')\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccaa72fa-f720-4c61-ad92-a771e07c9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    #drop missing va;ues since there is just 1 sample\n",
    "    data_nona = data.dropna()\n",
    "    #Normalization z-score\n",
    "    scaler = StandardScaler()    \n",
    "    # Store the columns to keep for later concatenation\n",
    "    columns_to_keep = ['participant', 'gender', 'depression']\n",
    "    # Extract the columns to be scaled and drop them from the original DataFrame\n",
    "    temp = data_nona[columns_to_keep].copy()\n",
    "    data_nona.drop(columns=columns_to_keep, axis=1, inplace=True)\n",
    "    # Scale the remaining columns using StandardScaler and convert back to DataFrame\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data_nona), columns=data_nona.columns)\n",
    "    scaled_data.reset_index(drop=True, inplace=True)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    # Concatenate the scaled data with the columns we kept earlier\n",
    "    processed_data = pd.concat([scaled_data, temp], axis=1)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afcae2-ac22-4117-a04e-9434806d7569",
   "metadata": {},
   "source": [
    "## Methods to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1453e6-d155-47fc-b17a-03d19987f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates accuracy\n",
    "# pass true and predicted labels\n",
    "# return accuracy score\n",
    "def calculate_total_accuracy(true_labels, predicted_labels):\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculates accuracy\n",
    "# pass true and predicted labels\n",
    "# return balanced accuracy score\n",
    "def calculate_balanced_accuracy(true_labels, predicted_labels):\n",
    "    #calculkate confusion matrix\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    TP = matrix[1, 1]\n",
    "    TN = matrix[0, 0]\n",
    "    FP = matrix[0, 1]\n",
    "    FN = matrix[1, 0]\n",
    "    #For positive class, how many correct predictions\n",
    "    accuracy_positive = TP/(TP+FN)\n",
    "    #For negative class how many \n",
    "    accuracy_negative = TN/(TN+FP)\n",
    "    return 0.5*(accuracy_positive + accuracy_negative)\n",
    "\n",
    "#Calculates Equality of Opportunity\n",
    "# pass true and predicted labels for male samples\n",
    "# pass true and predicted labels for female samples\n",
    "# return balanced accuracy score\n",
    "def calculate_EO(true_labels_male, \n",
    "                 true_labels_female,\n",
    "                 predicted_labels_male,\n",
    "                predicted_labels_female):\n",
    "    #Calculate True pistive rate for male gender with confusion matrix\n",
    "    matrix_male = confusion_matrix(true_labels_male, predicted_labels_male)\n",
    "    TP = matrix_male[1, 1]\n",
    "    TN = matrix_male[0, 0]\n",
    "    FN = matrix_male[1, 0]\n",
    "    TPR_male = TP/(TP+FN)\n",
    "\n",
    "    #Calculate True pistive rate for female gender with confusion matrix\n",
    "    matrix_female = confusion_matrix(true_labels_female, predicted_labels_female)\n",
    "    TP = matrix_female[1, 1]\n",
    "    TN = matrix_female[0, 0]\n",
    "    FN = matrix_female[1, 0]\n",
    "    TPR_female = TP/(TP+FN)\n",
    "    \n",
    "    # Calculate EO\n",
    "    return 1-abs(TPR_male-TPR_female) \n",
    "\n",
    "#Function to calculate majority votings\n",
    "#Pass labels\n",
    "#Returns mode or which label was predicted most\n",
    "def majority_voting(df):\n",
    "    counts = df.value_counts()\n",
    "    return counts.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63781b8a-f603-4075-b8ba-a252f66bac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate all metrics\n",
    "#Pass true labels, predicted labels and a reference(test/val) dataframe\n",
    "#referece dataframe should have all labels and features\n",
    "#Returns a dictionary with all the metric calculated\n",
    "def calculate_metrics(y_true, y_pred, test_data, EO=True):\n",
    "    # Initialize metrics\n",
    "    metrics = {}\n",
    "    #---------------------------------------------------CALCULATING TOTAL METRICS\n",
    "    #calculate total accuracy\n",
    "    metrics[\"Total accuracy\"] = calculate_total_accuracy(y_true, y_pred)\n",
    "    #calculate total balanced accuracy\n",
    "    metrics[\"Total Balanced accuracy\"] = calculate_balanced_accuracy(y_true, y_pred)\n",
    "    #calculate total EO\n",
    "    if(EO):\n",
    "        #find gender based indices for true labels from data\n",
    "        male_indices = test_data[test_data['gender']==1].index\n",
    "        female_indices = test_data[test_data['gender']==0].index\n",
    "        #separate true labels based on indices\n",
    "        male_true = y_true.loc[male_indices]\n",
    "        female_true = y_true.loc[female_indices]\n",
    "        #Find the corresponding indices for predicted labels from true_labels\n",
    "        male_true_index_list = male_true.index.tolist()\n",
    "        female_true_index_list = female_true.index.tolist()\n",
    "        #Get separated predicted labels based on gender\n",
    "        male_predicted = y_pred[[male_true_index_list.index(index) for index in male_true_index_list]]\n",
    "        female_predicted = y_pred[[female_true_index_list.index(index) for index in female_true_index_list]]\n",
    "        metrics[\"Total EO\"] = calculate_EO(male_true, female_true, male_predicted, female_predicted)\n",
    "    #-------------------------------------------------CALCULATING AGGREGATED METRICS FOR EACH PARTICIPANT\n",
    "    predictions_df = pd.DataFrame({'participant': test_data['participant'], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "    aggregated_y_true = predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "    aggregated_y_pred = predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "    #Calculate aggregated accuracy score\n",
    "    metrics[\"Aggregated accuracy score\"] = calculate_total_accuracy(aggregated_y_true, aggregated_y_pred)\n",
    "    #Calculate balanced aggregated accuracy\n",
    "    metrics[\"Aggregated balanced accuracy score\"] = calculate_balanced_accuracy(aggregated_y_true, aggregated_y_pred)\n",
    "    if(EO):\n",
    "        #Calculate aggregated EOs\n",
    "        male_predictions_df = pd.DataFrame({'participant': test_data['participant'].loc[male_indices], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "        male_aggregated_y_true = male_predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "        male_aggregated_y_pred = male_predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "        female_predictions_df = pd.DataFrame({'participant': test_data['participant'].loc[female_indices], 'predicted_label': y_pred, 'true_label': y_true})\n",
    "        female_aggregated_y_true = female_predictions_df.groupby('participant')['true_label'].agg(majority_voting)\n",
    "        female_aggregated_y_pred = female_predictions_df.groupby('participant')['predicted_label'].agg(majority_voting)\n",
    "        metrics[\"Aggregated EO score\"] = calculate_EO(male_aggregated_y_true, female_aggregated_y_true, male_aggregated_y_pred, female_aggregated_y_pred)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2503eab-929d-4284-ac8b-b6e01732ebed",
   "metadata": {},
   "source": [
    "## Data Modeling - Depression Classification\n",
    "### What models to try?\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- TBD..........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f0769-2394-44e0-a91e-535db7b7e2e8",
   "metadata": {},
   "source": [
    "### Model attempt: Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2c4d21-e943-4705-a523-ff1c9d7796cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for depth 3\n",
      "{'Total accuracy': 0.739302752293578, 'Total Balanced accuracy': 0.5909240479132576, 'Total EO': 0.9504593123805108, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5857142857142856, 'Aggregated EO score': 0.8333333333333334}\n",
      "\n",
      "for depth 5\n",
      "{'Total accuracy': 0.7472293577981651, 'Total Balanced accuracy': 0.6138374218217592, 'Total EO': 0.9324445056229891, 'Aggregated accuracy score': 0.7839080459770115, 'Aggregated balanced accuracy score': 0.6109126984126985, 'Aggregated EO score': 0.8833333333333332}\n",
      "\n",
      "for depth 7\n",
      "{'Total accuracy': 0.7634495412844038, 'Total Balanced accuracy': 0.6653390774441428, 'Total EO': 0.962120994404248, 'Aggregated accuracy score': 0.8160919540229885, 'Aggregated balanced accuracy score': 0.674404761904762, 'Aggregated EO score': 0.8166666666666667}\n",
      "\n",
      "for depth 9\n",
      "{'Total accuracy': 0.7696146788990825, 'Total Balanced accuracy': 0.6867998981419393, 'Total EO': 0.9746201537650562, 'Aggregated accuracy score': 0.8367816091954022, 'Aggregated balanced accuracy score': 0.717063492063492, 'Aggregated EO score': 0.8666666666666666}\n",
      "\n",
      "for depth 15\n",
      "{'Total accuracy': 0.7605871559633027, 'Total Balanced accuracy': 0.7057221489037487, 'Total EO': 0.9818361595826637, 'Aggregated accuracy score': 0.9057471264367816, 'Aggregated balanced accuracy score': 0.8369047619047618, 'Aggregated EO score': 0.8666666666666668}\n",
      "\n",
      "for depth 30\n",
      "{'Total accuracy': 0.7552293577981652, 'Total Balanced accuracy': 0.7058031055863679, 'Total EO': 0.9779235696944484, 'Aggregated accuracy score': 0.9126436781609195, 'Aggregated balanced accuracy score': 0.8545634920634921, 'Aggregated EO score': 0.8833333333333334}\n",
      "\n",
      "for depth 50\n",
      "{'Total accuracy': 0.7558899082568807, 'Total Balanced accuracy': 0.70621896934617, 'Total EO': 0.9780648348221442, 'Aggregated accuracy score': 0.9218390804597701, 'Aggregated balanced accuracy score': 0.8660714285714286, 'Aggregated EO score': 0.9166666666666667}\n",
      "\n",
      "for depth 70\n",
      "{'Total accuracy': 0.7516330275229358, 'Total Balanced accuracy': 0.702200767352813, 'Total EO': 0.9820057038545433, 'Aggregated accuracy score': 0.9126436781609195, 'Aggregated balanced accuracy score': 0.8494047619047619, 'Aggregated EO score': 0.95}\n",
      "\n",
      "for depth 90\n",
      "{'Total accuracy': 0.756770642201835, 'Total Balanced accuracy': 0.7054229390843532, 'Total EO': 0.981962576782965, 'Aggregated accuracy score': 0.9172413793103449, 'Aggregated balanced accuracy score': 0.8551587301587302, 'Aggregated EO score': 0.9000000000000001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------Tuning for different depths-------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "training_data = preprocess_data(training_df)\n",
    "X = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y = training_data['depression']\n",
    "\n",
    "# Define the depths to experiment with\n",
    "depths = [3, 5, 7, 9, 15, 30, 50, 70, 90]\n",
    "# Initialize metrics\n",
    "metrics = {}\n",
    "\n",
    "# Perform cross-validation for each tree depth\n",
    "for depth in depths:\n",
    "    # Initialize decision tree model\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "    # Cross validation k fold, 4:1::training:validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #list to store metrics for each cross validation split\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Perform cross-validation and collect metrics\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        #Fit the training set\n",
    "        tree.fit(X_train, y_train)\n",
    "        #Predict validation set\n",
    "        y_pred = tree.predict(X_val)\n",
    "        #calculate metrics\n",
    "        fold_metrics.append(calculate_metrics(y_val, y_pred, training_data.iloc[val_index]))\n",
    "\n",
    "    #find avg metrics for each depth\n",
    "    metrics[depth] = {}\n",
    "    sums = {}\n",
    "    for metric in fold_metrics:\n",
    "        for key, value in metric.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    for key in sums:\n",
    "        sums[key]/=len(fold_metrics)\n",
    "    metrics[depth] = sums\n",
    "\n",
    "#Print metrics for all the hyperparameters (Depth)\n",
    "for depth in metrics:\n",
    "    print(f\"for depth {depth}\")\n",
    "    print(metrics[depth])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f350c55a-6fa3-45e4-8c4e-8023129d953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total accuracy': 0.699390243902439, 'Total Balanced accuracy': 0.5916876320821174, 'Total EO': 0.7163647215785408, 'Aggregated accuracy score': 0.7, 'Aggregated balanced accuracy score': 0.5476190476190477, 'Aggregated EO score': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------Testing for best depth----------------------------------------------------------------\n",
    "test_data = preprocess_data(test_df)\n",
    "training_data = preprocess_data(training_df)\n",
    "X_train = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_train = training_data['depression']\n",
    "X_test = test_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_test = test_data['depression']\n",
    "\n",
    "best_depth = 70\n",
    "# Initialize decision tree model\n",
    "tree = DecisionTreeClassifier(max_depth=best_depth, criterion='entropy')\n",
    "#Fit the training set\n",
    "tree.fit(X_train, y_train)\n",
    "#Predict for test set\n",
    "y_pred = tree.predict(X_test)\n",
    "#get metrics\n",
    "metrics = calculate_metrics(y_test, y_pred, test_data)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd8a4b-eb5a-4222-8cf0-9ec59f3177e6",
   "metadata": {},
   "source": [
    "## Model attempt: Logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224d0b90-adc4-4685-a3c0-673dc06127ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 1.0, 'solver': 'liblinear'}\n",
      "{'Total accuracy': 0.7428256880733944, 'Total Balanced accuracy': 0.5884060803856384, 'Total EO': 0.9477892614269802, 'Aggregated accuracy score': 0.7609195402298852, 'Aggregated balanced accuracy score': 0.5666666666666667, 'Aggregated EO score': 0.9333333333333332}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear'}\n",
      "{'Total accuracy': 0.7423119266055046, 'Total Balanced accuracy': 0.5882667934329493, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n",
      "{'penalty': 'l2', 'C': 0.99, 'solver': 'liblinear'}\n",
      "{'Total accuracy': 0.7422385321100918, 'Total Balanced accuracy': 0.5882157209099668, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n",
      "{'penalty': 'l2', 'C': 0.95, 'solver': 'liblinear'}\n",
      "{'Total accuracy': 0.7422385321100918, 'Total Balanced accuracy': 0.5882157209099668, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n",
      "{'penalty': 'l2', 'C': 0.9, 'solver': 'liblinear'}\n",
      "{'Total accuracy': 0.7423119266055046, 'Total Balanced accuracy': 0.5881951199255223, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'lbfgs'}\n",
      "{'Total accuracy': 0.7422385321100917, 'Total Balanced accuracy': 0.5879845089943649, 'Total EO': 0.9467826783811434, 'Aggregated accuracy score': 0.7563218390804598, 'Aggregated balanced accuracy score': 0.5583333333333333, 'Aggregated EO score': 0.9333333333333332}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'sag'}\n",
      "{'Total accuracy': 0.7383486238532109, 'Total Balanced accuracy': 0.5795665708266226, 'Total EO': 0.9388864946436513, 'Aggregated accuracy score': 0.7563218390804598, 'Aggregated balanced accuracy score': 0.5583333333333333, 'Aggregated EO score': 0.9333333333333332}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'saga'}\n",
      "{'Total accuracy': 0.7377614678899083, 'Total Balanced accuracy': 0.5756836852328879, 'Total EO': 0.9393244676712401, 'Aggregated accuracy score': 0.7517241379310344, 'Aggregated balanced accuracy score': 0.5499999999999999, 'Aggregated EO score': 0.9333333333333332}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'newton-cg'}\n",
      "{'Total accuracy': 0.7423119266055046, 'Total Balanced accuracy': 0.588190534202405, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n",
      "{'penalty': 'l2', 'C': 1.0, 'solver': 'newton-cholesky'}\n",
      "{'Total accuracy': 0.7423853211009174, 'Total Balanced accuracy': 0.5883189039071547, 'Total EO': 0.9466991528254788, 'Aggregated accuracy score': 0.7586206896551724, 'Aggregated balanced accuracy score': 0.5625, 'Aggregated EO score': 0.9166666666666666}\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "training_data = preprocess_data(training_df)\n",
    "X = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y = training_data['depression']\n",
    "\n",
    "hyperparams = [\n",
    "    {'penalty': 'l1', 'C': 1.0, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 0.99, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 0.95, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 0.9, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'lbfgs'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'sag'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'saga'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'newton-cg'},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'newton-cholesky'},\n",
    "]\n",
    "metrics = {}\n",
    "\n",
    "for hyperparam in hyperparams:\n",
    "    print(hyperparam)\n",
    "    # Initialize Logistic regression model\n",
    "    model = LogisticRegression(solver=hyperparam['solver'], C=hyperparam['C'], penalty=hyperparam['penalty'])\n",
    "    # Cross validation k fold, 4:1::training:validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #list to store metrics for each cross validation split\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Perform cross-validation and collect metrics\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "        #Fit the training set\n",
    "        model.fit(X_train, y_train)\n",
    "        #Predict validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        #calculate metrics\n",
    "        fold_metrics.append(calculate_metrics(y_val, y_pred, training_data.iloc[val_index]))\n",
    "    \n",
    "    #find avg metrics for each depth\n",
    "    # metrics[depth] = {}\n",
    "    sums = {}\n",
    "    for metric in fold_metrics:\n",
    "        for key, value in metric.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    for key in sums:\n",
    "        sums[key]/=len(fold_metrics)\n",
    "    print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d2085a-fc61-4f72-9b1d-61d51bb9c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total accuracy': 0.761890243902439, 'Total Balanced accuracy': 0.5125821245273796, 'Total EO': 0.9488448844884488, 'Aggregated accuracy score': 0.7, 'Aggregated balanced accuracy score': 0.5, 'Aggregated EO score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------Testing for best hyperparameter----------------------------------------------------------------\n",
    "test_data = preprocess_data(test_df)\n",
    "training_data = preprocess_data(training_df)\n",
    "X_train = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_train = training_data['depression']\n",
    "X_test = test_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_test = test_data['depression']\n",
    "\n",
    "best_depth = 70\n",
    "# Initialize decision tree model\n",
    "model = LogisticRegression(penalty='l1', C=1.0, solver='liblinear')\n",
    "#Fit the training set\n",
    "model.fit(X_train, y_train)\n",
    "#Predict for test set\n",
    "y_pred = model.predict(X_test)\n",
    "#get metrics\n",
    "metrics = calculate_metrics(y_test, y_pred, test_data)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db6b28-59d3-4cf1-8f0d-bc2bad32ec8c",
   "metadata": {},
   "source": [
    "## Gender Classification\n",
    "\n",
    "### Model attempt: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b20b755e-79fe-4ae6-8248-c1e4979b47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for depth 3\n",
      "{'Total accuracy': 0.9236697247706422, 'Total Balanced accuracy': 0.9192004514666632, 'Aggregated accuracy score': 0.9862068965517242, 'Aggregated balanced accuracy score': 0.9841503267973856}\n",
      "\n",
      "for depth 5\n",
      "{'Total accuracy': 0.9285871559633027, 'Total Balanced accuracy': 0.9223470848664723, 'Aggregated accuracy score': 0.993103448275862, 'Aggregated balanced accuracy score': 0.9916666666666668}\n",
      "\n",
      "for depth 7\n",
      "{'Total accuracy': 0.9289541284403671, 'Total Balanced accuracy': 0.9258356190184619, 'Aggregated accuracy score': 0.9954022988505746, 'Aggregated balanced accuracy score': 0.9944444444444445}\n",
      "\n",
      "for depth 9\n",
      "{'Total accuracy': 0.9257981651376147, 'Total Balanced accuracy': 0.9229877129799933, 'Aggregated accuracy score': 0.9954022988505746, 'Aggregated balanced accuracy score': 0.9944444444444445}\n",
      "\n",
      "for depth 15\n",
      "{'Total accuracy': 0.9225688073394496, 'Total Balanced accuracy': 0.9193688045774605, 'Aggregated accuracy score': 0.993103448275862, 'Aggregated balanced accuracy score': 0.9916666666666668}\n",
      "\n",
      "for depth 30\n",
      "{'Total accuracy': 0.9235963302752295, 'Total Balanced accuracy': 0.9206193229830142, 'Aggregated accuracy score': 0.993103448275862, 'Aggregated balanced accuracy score': 0.9916666666666668}\n",
      "\n",
      "for depth 50\n",
      "{'Total accuracy': 0.9230825688073395, 'Total Balanced accuracy': 0.9201061898796894, 'Aggregated accuracy score': 0.993103448275862, 'Aggregated balanced accuracy score': 0.9916666666666668}\n",
      "\n",
      "for depth 70\n",
      "{'Total accuracy': 0.9238165137614679, 'Total Balanced accuracy': 0.9207927653888763, 'Aggregated accuracy score': 0.9954022988505746, 'Aggregated balanced accuracy score': 0.9944444444444445}\n",
      "\n",
      "for depth 90\n",
      "{'Total accuracy': 0.9226422018348623, 'Total Balanced accuracy': 0.9197817695992014, 'Aggregated accuracy score': 0.9954022988505746, 'Aggregated balanced accuracy score': 0.9944444444444445}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------Tuning for different depths-------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "training_data = preprocess_data(training_df)\n",
    "X = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y = training_data['gender']\n",
    "\n",
    "# Define the depths to experiment with\n",
    "depths = [3, 5, 7, 9, 15, 30, 50, 70, 90]\n",
    "# Initialize metrics\n",
    "metrics = {}\n",
    "\n",
    "# Perform cross-validation for each tree depth\n",
    "for depth in depths:\n",
    "    # Initialize decision tree model\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "    # Cross validation k fold, 4:1::training:validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #list to store metrics for each cross validation split\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Perform cross-validation and collect metrics\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        #Fit the training set\n",
    "        tree.fit(X_train, y_train)\n",
    "        #Predict validation set\n",
    "        y_pred = tree.predict(X_val)\n",
    "        #calculate metrics\n",
    "        fold_metrics.append(calculate_metrics(y_val, y_pred, training_data.iloc[val_index], False))\n",
    "\n",
    "    #find avg metrics for each depth\n",
    "    metrics[depth] = {}\n",
    "    sums = {}\n",
    "    for metric in fold_metrics:\n",
    "        for key, value in metric.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    for key in sums:\n",
    "        sums[key]/=len(fold_metrics)\n",
    "    metrics[depth] = sums\n",
    "\n",
    "#Print metrics for all the hyperparameters (Depth)\n",
    "for depth in metrics:\n",
    "    print(f\"for depth {depth}\")\n",
    "    print(metrics[depth])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35cad93c-f60c-47b3-b719-6c5da14f245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total accuracy': 0.8655487804878049, 'Total Balanced accuracy': 0.8616732045232312, 'Aggregated accuracy score': 0.95, 'Aggregated balanced accuracy score': 0.9375}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------Testing for best depth----------------------------------------------------------------\n",
    "test_data = preprocess_data(test_df)\n",
    "training_data = preprocess_data(training_df)\n",
    "X_train = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_train = training_data['gender']\n",
    "X_test = test_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y_test = test_data['gender']\n",
    "\n",
    "best_depth = 70\n",
    "# Initialize decision tree model\n",
    "tree = DecisionTreeClassifier(max_depth=best_depth, criterion='entropy')\n",
    "#Fit the training set\n",
    "tree.fit(X_train, y_train)\n",
    "#Predict for test set\n",
    "y_pred = tree.predict(X_test)\n",
    "#get metrics\n",
    "metrics = calculate_metrics(y_test, y_pred, test_data, False)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476048a7-543d-416e-b728-aa21f8bf040b",
   "metadata": {},
   "source": [
    "## Gender classification model attempt: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af3ad6-c24b-4e70-b1d4-02efb08bd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------Tuning for different depths-------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "training_data = preprocess_data(training_df)\n",
    "X = training_data.drop(columns = ['participant', 'gender', 'depression'], axis=1)\n",
    "y = training_data['gender']\n",
    "\n",
    "# Define the depths to experiment with\n",
    "depths = [3, 5, 7, 9, 15, 30, 50, 70, 90]\n",
    "# Initialize metrics\n",
    "metrics = {}\n",
    "\n",
    "# Perform cross-validation for each tree depth\n",
    "for depth in depths:\n",
    "    # Initialize decision tree model\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "    # Cross validation k fold, 4:1::training:validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #list to store metrics for each cross validation split\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Perform cross-validation and collect metrics\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        #Fit the training set\n",
    "        tree.fit(X_train, y_train)\n",
    "        #Predict validation set\n",
    "        y_pred = tree.predict(X_val)\n",
    "        #calculate metrics\n",
    "        fold_metrics.append(calculate_metrics(y_val, y_pred, training_data.iloc[val_index], False))\n",
    "\n",
    "    #find avg metrics for each depth\n",
    "    metrics[depth] = {}\n",
    "    sums = {}\n",
    "    for metric in fold_metrics:\n",
    "        for key, value in metric.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    for key in sums:\n",
    "        sums[key]/=len(fold_metrics)\n",
    "    metrics[depth] = sums\n",
    "\n",
    "#Print metrics for all the hyperparameters (Depth)\n",
    "for depth in metrics:\n",
    "    print(f\"for depth {depth}\")\n",
    "    print(metrics[depth])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
